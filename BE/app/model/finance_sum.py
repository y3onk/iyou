# batch_process_summaries.py (IDë³„ ì²˜ë¦¬ ê¸°ëŠ¥ ì¶”ê°€ ìµœì¢… ë²„ì „)

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import kss
import sqlite3
from transformers import BertModel
from kobert_tokenizer import KoBERTTokenizer
from konlpy.tag import Okt # Mecab ëŒ€ì‹  Okt ì‚¬ìš©
from collections import Counter
from openai import OpenAI
from tqdm import tqdm
import logging
import argparse # [ì¶”ê°€] ì»¤ë§¨ë“œ ë¼ì¸ ì¸ì ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from dotenv import load_dotenv
import json
#load_dotenv()

# --- ê¸°ë³¸ ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- ì„¤ì • (Configuration) ---
BASE_MODEL_NAME = "skt/kobert-base-v1"

# (ì´ì „ê³¼ ë™ì¼í•œ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ ì •ì˜ëŠ” ìƒëµ)
# ... BERTClassifier, BERTSumDataset, extract_key_sentences, extract_keywords, generate_gpt_summary ...
class BERTClassifier(nn.Module):
    def __init__(self, bert, hidden_size=768, num_classes=2, dr_rate=None):
        super().__init__()
        self.bert = bert
        self.classifier = nn.Linear(hidden_size, num_classes)
        self.dr_rate = dr_rate
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)

    def forward(self, input_ids, attention_mask, token_type_ids=None):
        _, pooler_output = self.bert(input_ids=input_ids,
                                 attention_mask=attention_mask,
                                 token_type_ids=token_type_ids)
        out = self.dropout(pooler_output) if self.dr_rate and self.training else pooler_output
        return self.classifier(out)


def extract_key_sentences(text: str, model: BERTClassifier, tokenizer, device, top_n: int = 3) -> list[str]:
    with torch.no_grad():
        sentences = kss.split_sentences(text)
        if not sentences: return []
        inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors="pt", max_length=128).to(device)
        outputs = model(**inputs); probs = F.softmax(outputs, dim=1); core_probs = probs[:, 1].cpu().numpy()
        sorted_indices = core_probs.argsort()[::-1]
        return [sentences[idx] for idx in sorted_indices[:top_n]]
def extract_keywords(sentences_list: list[str], tagger) -> list[str]:
    full_text = " ".join(sentences_list); nouns = tagger.nouns(full_text)
    meaningful_nouns = [n for n in nouns if len(n) > 1]
    if not meaningful_nouns: return []
    return list(Counter(meaningful_nouns).keys())

def extract_keywords(sentences_list: list[str], tagger) -> list[str]:
    full_text = " ".join(sentences_list)
    nouns = tagger.nouns(full_text)
    meaningful_nouns = [n for n in nouns if len(n) > 1]
    return list(Counter(meaningful_nouns).keys())

# --- GPT ìš”ì•½ ìƒì„± ---
def generate_gpt_summary(key_sentences: list[str], raw_keywords: list[str], client: OpenAI) -> str:
    if not key_sentences: return ""
    prompt = f"""# ì§€ì‹œì‚¬í•­
ë‹¹ì‹ ì€ ê¸ˆìœµ ë° ë²•ë¥  ì•½ê´€ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•˜ëŠ” ìµœê³  ìˆ˜ì¤€ì˜ AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
1ì°¨ AIê°€ ì¶”ì¶œí•œ 'í•µì‹¬ ë¬¸ì¥'ê³¼ 'ì´ˆë²Œ í‚¤ì›Œë“œ'ê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ **JSON í˜•ì‹**ìœ¼ë¡œ ì¶œë ¥í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

1.  **`refined_keywords`**: 'í•µì‹¬ ë¬¸ì¥'ì˜ ë¬¸ë§¥ì„ ê¹Šì´ ì´í•´í•˜ì—¬, 'ì´ˆë²Œ í‚¤ì›Œë“œ' ì¤‘ì—ì„œ ì •ë§ë¡œ ì¤‘ìš”í•˜ê³  ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ë§Œ í•„í„°ë§í•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸. ë¶ˆí•„ìš”í•˜ê±°ë‚˜, í˜•íƒœì†Œê°€ ì´ìƒí•˜ê±°ë‚˜, ì¤‘ìš”í•˜ì§€ ì•Šì€ ë‹¨ì–´ëŠ” ë°˜ë“œì‹œ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤.
2.  **`summary_text`**: 'í•µì‹¬ ë¬¸ì¥'ì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³ , ë‹¹ì‹ ì´ ë°©ê¸ˆ ì •ì œí•œ `refined_keywords`ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•˜ì—¬, ìµœì¢… ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ 3ë¬¸ì¥ ì´ë‚´ì˜ ì™„ê²°ëœ ìš”ì•½ë¬¸.

# ì œì•½ ì¡°ê±´
- `summary_text`ëŠ” 'í•µì‹¬ ë¬¸ì¥'ì— ì—†ëŠ” ë‚´ìš©ì„ ì ˆëŒ€ ì¶”ê°€í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
- ì›ë¬¸ì˜ ì „ë¬¸ì ì¸ ì–´íœ˜ì™€ ì˜ë¯¸ë¥¼ ìµœëŒ€í•œ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.
- ìµœì¢… ì¶œë ¥ì€ ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

# ì…ë ¥ ë°ì´í„°
## í•µì‹¬ ë¬¸ì¥
{'- ' + '\n- '.join(key_sentences)}

## ì´ˆë²Œ í‚¤ì›Œë“œ
{', '.join(raw_keywords)}

# ì¶œë ¥ (JSON í˜•ì‹):
"""
    try:
        completion = client.chat.completions.create(
            model="gpt-4o",
            response_format={"type": "json_object"}, # JSON ì¶œë ¥ ëª¨ë“œ ì‚¬ìš©
            messages=[{"role": "user", "content": prompt}]
        )
        result_json = json.loads(completion.choices[0].message.content)
        
        refined_keywords = result_json.get("refined_keywords", raw_keywords)
        summary_text = result_json.get("summary_text", " ".join(key_sentences))
        
        return summary_text, refined_keywords

    except Exception as e:
        logging.error(f"GPT API í˜¸ì¶œ ë˜ëŠ” JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ, ì •ì œ ì—†ì´ ê¸°ì¡´ ê²°ê³¼ ë°˜í™˜
        return " ".join(key_sentences), raw_keywords
        
        
    
# --- [ìˆ˜ì •ë¨] DB ì—°ë™ í•¨ìˆ˜ ---
def get_all_terms(DB_PATH: str):
    """ DBì—ì„œ 'ëª¨ë“ ' ì•½ê´€ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ """
    with sqlite3.connect(DB_PATH) as conn:
        cur = conn.cursor()
        cur.execute("SELECT id, title, content FROM terms")
        return cur.fetchall()

def get_terms_by_ids(DB_PATH: str, term_ids: list[int]):
    """ [ì¶”ê°€ë¨] DBì—ì„œ 'ì§€ì •ëœ ID'ì˜ ì•½ê´€ë“¤ë§Œ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ """
    with sqlite3.connect(DB_PATH) as conn:
        cur = conn.cursor()
        # SQLì˜ IN ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ IDë¥¼ í•œ ë²ˆì— ì¡°íšŒ
        placeholders = ','.join('?' for _ in term_ids)
        query = f"SELECT id, title, content FROM terms WHERE id IN ({placeholders})"
        cur.execute(query, term_ids)
        return cur.fetchall()

def save_summary_to_db(DB_PATH: str, term_id: int, summary_text: str, keywords: list[str]):
    """
    term_summaries í…Œì´ë¸”ì— ìš”ì•½ ì €ì¥
    revision_versionì€ ë™ì¼ term_id ë‚´ì—ì„œ ì¡´ì¬í•˜ëŠ” ë²„ì „ì„ ê¸°ì¤€ìœ¼ë¡œ ìë™ ìƒì„±
    """
    keywords_str = ','.join(keywords)

    with sqlite3.connect(DB_PATH) as conn:
        cur = conn.cursor()
        # í…Œì´ë¸” ìƒì„± (ì—†ìœ¼ë©´)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS term_summaries (
                id               INTEGER      NOT NULL PRIMARY KEY AUTOINCREMENT,
                term_id          INTEGER      NOT NULL REFERENCES terms (id) ON DELETE CASCADE,
                revision_version VARCHAR(50)  NOT NULL,
                summary_text     TEXT,
                created_at       DATETIME     NOT NULL DEFAULT CURRENT_TIMESTAMP,
                keywords         TEXT,
                CONSTRAINT uq_term_revision UNIQUE(term_id, revision_version)
            )
        """)
        cur.execute("CREATE INDEX IF NOT EXISTS ix_term_summaries_term_id ON term_summaries (term_id)")

        # ê¸°ì¡´ revision_version ì¡°íšŒ
        cur.execute("""
            SELECT revision_version
            FROM term_summaries
            WHERE term_id = ?
        """, (term_id,))
        existing_versions = cur.fetchall()  # [('v1',), ('v2',), ...]

        # ë‹¤ìŒ ë²„ì „ ê³„ì‚°
        if not existing_versions:
            next_version = 1
        else:
            # 'vìˆ«ì' í˜•íƒœì—ì„œ ìˆ«ìë§Œ ë½‘ì•„ ìµœëŒ€ê°’ + 1
            max_v = max(int(v[0].lstrip('v')) for v in existing_versions)
            next_version = max_v + 1

        revision_version = f"v{next_version}"

        # ë°ì´í„° ì‚½ì…
        cur.execute("""
            INSERT INTO term_summaries (term_id, revision_version, summary_text, keywords)
            VALUES (?, ?, ?, ?)
        """, (term_id, revision_version, summary_text, keywords_str))
        conn.commit()
# ========================================================================================
# 4. ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ========================================================================================
if __name__ == "__main__":
    # --- [ìˆ˜ì •ë¨] ì»¤ë§¨ë“œ ë¼ì¸ ì¸ì íŒŒì„œ ì„¤ì • ---
    load_dotenv()
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
    PROJECT_ROOT = os.path.dirname(os.path.dirname(SCRIPT_DIR))
    DOTENV_PATH = os.path.join(PROJECT_ROOT, '.env')

    DB_URL = os.getenv("DATABASE_URL", "term.db")  # sqlite:./term.db
    if DB_URL.startswith("sqlite:"):
        DB_PATH = DB_URL.replace("sqlite:", "")      # ./term.db
        DB_PATH = os.path.abspath(os.path.join(PROJECT_ROOT, DB_PATH))
    else:
        DB_PATH = os.path.abspath(DB_URL)

    print("DB_PATH:", DB_PATH)

    MODEL_PATH = os.path.join(SCRIPT_DIR, "kobert_summarization_model.pth")
    
    logging.info(f".env íŒŒì¼ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    logging.info(f"DB ê²½ë¡œ: {DB_PATH}")
    logging.info(f"ëª¨ë¸ ê²½ë¡œ: {MODEL_PATH}")

    # db_url = os.getenv("DATABASE_URL")
    # if not db_url:
    #     raise ValueError(".env íŒŒì¼ì— DATABASE_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    # DB_PATH = os.path.join(PROJECT_ROOT, db_url) 

    #MODEL_PATH = os.path.join(SCRIPT_DIR, "kobert_summarization_model.pth")

    parser = argparse.ArgumentParser(description="ì•½ê´€ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ê³  í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ DBì— ì €ì¥í•˜ëŠ” ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--ids",
        nargs="+",  # ì—¬ëŸ¬ ê°œì˜ IDë¥¼ ë°›ì„ ìˆ˜ ìˆë„ë¡ ì„¤ì •
        type=int,
        help="ì²˜ë¦¬í•  íŠ¹ì • ì•½ê´€ ID ëª©ë¡. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ DBì˜ ëª¨ë“  ì•½ê´€ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."
    )
    args = parser.parse_args()

    # --- 1. AI ëª¨ë¸ ë° ë¦¬ì†ŒìŠ¤ ë¡œë“œ ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"Using device: {device}")
    
    try:
        tokenizer = KoBERTTokenizer.from_pretrained(BASE_MODEL_NAME)
        bertmodel = BertModel.from_pretrained(BASE_MODEL_NAME, return_dict=False)
        summarization_model = BERTClassifier(bertmodel, num_classes=2, dr_rate=0.5).to(device)
        summarization_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
        summarization_model.eval()
        logging.info("âœ… ìµœì¢… ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        logging.error(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        exit()
    

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logging.warning("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GPT ìš”ì•½ì€ ì¶”ì¶œ ë¬¸ì¥ ì¡°í•©ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.")
    gpt_client = OpenAI(api_key=api_key)
    okt_tagger = Okt()

    # --- [ìˆ˜ì •ë¨] 2. ì¸ìì— ë”°ë¼ DBì—ì„œ ì•½ê´€ ì›ë¬¸ ë¶ˆëŸ¬ì˜¤ê¸° ---
    if args.ids:
        # --ids ì¸ìê°€ ì£¼ì–´ì¡Œìœ¼ë©´, í•´ë‹¹ IDì˜ ì•½ê´€ë§Œ ë¶ˆëŸ¬ì˜¤ê¸°
        terms_to_process = get_terms_by_ids(DB_PATH, args.ids)
        logging.info(f"ì§€ì •ëœ {len(terms_to_process)}ê°œì˜ ì•½ê´€ì— ëŒ€í•´ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤: {args.ids}")
    else:
        # --ids ì¸ìê°€ ì—†ìœ¼ë©´, ëª¨ë“  ì•½ê´€ ë¶ˆëŸ¬ì˜¤ê¸°
        terms_to_process = get_all_terms(DB_PATH)
        logging.info(f"ì´ {len(terms_to_process)}ê°œì˜ ëª¨ë“  ì•½ê´€ì— ëŒ€í•´ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

    # --- 3. ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰ ---
    # --- ë°°ì¹˜ ì²˜ë¦¬ ---
    if not terms_to_process:
        logging.warning("ì²˜ë¦¬í•  ì•½ê´€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for term_id, title, content in tqdm(terms_to_process, desc="ì „ì²´ ì•½ê´€ ìš”ì•½ ì²˜ë¦¬ ì¤‘"):
            key_sentences = extract_key_sentences(content, summarization_model, tokenizer, device, top_n=3)
            keywords = extract_keywords(key_sentences, okt_tagger)
            final_summary, refined_keywords = generate_gpt_summary(key_sentences, keywords, gpt_client)
            save_summary_to_db(DB_PATH, term_id, final_summary, refined_keywords)

    logging.info("ğŸ‰ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")